Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:06<01:35,  6.79s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:13<01:27,  6.74s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:20<01:21,  6.78s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:26<01:13,  6.72s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:34<01:08,  6.84s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:40<01:00,  6.75s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:47<00:54,  6.77s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:54<00:47,  6.72s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:01<00:40,  6.82s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:07<00:33,  6.78s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:14<00:27,  6.82s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:21<00:20,  6.73s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:28<00:13,  6.80s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:34<00:06,  6.73s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  5.38s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:36<00:00,  6.46s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at results_dump/edited_models_dump/llama3_70B_instruct_seed_42_top_48_heads_alpha_15 and are newly initialized: ['model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.40.self_attn.k_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.40.self_attn.q_proj.bias', 'model.layers.40.self_attn.v_proj.bias', 'model.layers.41.self_attn.k_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.41.self_attn.q_proj.bias', 'model.layers.41.self_attn.v_proj.bias', 'model.layers.42.self_attn.k_proj.bias', 'model.layers.42.self_attn.o_proj.bias', 'model.layers.42.self_attn.q_proj.bias', 'model.layers.42.self_attn.v_proj.bias', 'model.layers.43.self_attn.k_proj.bias', 'model.layers.43.self_attn.o_proj.bias', 'model.layers.43.self_attn.q_proj.bias', 'model.layers.43.self_attn.v_proj.bias', 'model.layers.44.self_attn.k_proj.bias', 'model.layers.44.self_attn.o_proj.bias', 'model.layers.44.self_attn.q_proj.bias', 'model.layers.44.self_attn.v_proj.bias', 'model.layers.45.self_attn.k_proj.bias', 'model.layers.45.self_attn.o_proj.bias', 'model.layers.45.self_attn.q_proj.bias', 'model.layers.45.self_attn.v_proj.bias', 'model.layers.46.self_attn.k_proj.bias', 'model.layers.46.self_attn.o_proj.bias', 'model.layers.46.self_attn.q_proj.bias', 'model.layers.46.self_attn.v_proj.bias', 'model.layers.47.self_attn.k_proj.bias', 'model.layers.47.self_attn.o_proj.bias', 'model.layers.47.self_attn.q_proj.bias', 'model.layers.47.self_attn.v_proj.bias', 'model.layers.48.self_attn.k_proj.bias', 'model.layers.48.self_attn.o_proj.bias', 'model.layers.48.self_attn.q_proj.bias', 'model.layers.48.self_attn.v_proj.bias', 'model.layers.49.self_attn.k_proj.bias', 'model.layers.49.self_attn.o_proj.bias', 'model.layers.49.self_attn.q_proj.bias', 'model.layers.49.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.50.self_attn.k_proj.bias', 'model.layers.50.self_attn.o_proj.bias', 'model.layers.50.self_attn.q_proj.bias', 'model.layers.50.self_attn.v_proj.bias', 'model.layers.51.self_attn.k_proj.bias', 'model.layers.51.self_attn.o_proj.bias', 'model.layers.51.self_attn.q_proj.bias', 'model.layers.51.self_attn.v_proj.bias', 'model.layers.52.self_attn.k_proj.bias', 'model.layers.52.self_attn.o_proj.bias', 'model.layers.52.self_attn.q_proj.bias', 'model.layers.52.self_attn.v_proj.bias', 'model.layers.53.self_attn.k_proj.bias', 'model.layers.53.self_attn.o_proj.bias', 'model.layers.53.self_attn.q_proj.bias', 'model.layers.53.self_attn.v_proj.bias', 'model.layers.54.self_attn.k_proj.bias', 'model.layers.54.self_attn.o_proj.bias', 'model.layers.54.self_attn.q_proj.bias', 'model.layers.54.self_attn.v_proj.bias', 'model.layers.55.self_attn.k_proj.bias', 'model.layers.55.self_attn.o_proj.bias', 'model.layers.55.self_attn.q_proj.bias', 'model.layers.55.self_attn.v_proj.bias', 'model.layers.56.self_attn.k_proj.bias', 'model.layers.56.self_attn.o_proj.bias', 'model.layers.56.self_attn.q_proj.bias', 'model.layers.56.self_attn.v_proj.bias', 'model.layers.57.self_attn.k_proj.bias', 'model.layers.57.self_attn.o_proj.bias', 'model.layers.57.self_attn.q_proj.bias', 'model.layers.57.self_attn.v_proj.bias', 'model.layers.58.self_attn.k_proj.bias', 'model.layers.58.self_attn.o_proj.bias', 'model.layers.58.self_attn.q_proj.bias', 'model.layers.58.self_attn.v_proj.bias', 'model.layers.59.self_attn.k_proj.bias', 'model.layers.59.self_attn.o_proj.bias', 'model.layers.59.self_attn.q_proj.bias', 'model.layers.59.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.60.self_attn.k_proj.bias', 'model.layers.60.self_attn.o_proj.bias', 'model.layers.60.self_attn.q_proj.bias', 'model.layers.60.self_attn.v_proj.bias', 'model.layers.61.self_attn.k_proj.bias', 'model.layers.61.self_attn.o_proj.bias', 'model.layers.61.self_attn.q_proj.bias', 'model.layers.61.self_attn.v_proj.bias', 'model.layers.62.self_attn.k_proj.bias', 'model.layers.62.self_attn.o_proj.bias', 'model.layers.62.self_attn.q_proj.bias', 'model.layers.62.self_attn.v_proj.bias', 'model.layers.63.self_attn.k_proj.bias', 'model.layers.63.self_attn.o_proj.bias', 'model.layers.63.self_attn.q_proj.bias', 'model.layers.63.self_attn.v_proj.bias', 'model.layers.64.self_attn.k_proj.bias', 'model.layers.64.self_attn.o_proj.bias', 'model.layers.64.self_attn.q_proj.bias', 'model.layers.64.self_attn.v_proj.bias', 'model.layers.65.self_attn.k_proj.bias', 'model.layers.65.self_attn.o_proj.bias', 'model.layers.65.self_attn.q_proj.bias', 'model.layers.65.self_attn.v_proj.bias', 'model.layers.66.self_attn.k_proj.bias', 'model.layers.66.self_attn.o_proj.bias', 'model.layers.66.self_attn.q_proj.bias', 'model.layers.66.self_attn.v_proj.bias', 'model.layers.67.self_attn.k_proj.bias', 'model.layers.67.self_attn.o_proj.bias', 'model.layers.67.self_attn.q_proj.bias', 'model.layers.67.self_attn.v_proj.bias', 'model.layers.68.self_attn.k_proj.bias', 'model.layers.68.self_attn.o_proj.bias', 'model.layers.68.self_attn.q_proj.bias', 'model.layers.68.self_attn.v_proj.bias', 'model.layers.69.self_attn.k_proj.bias', 'model.layers.69.self_attn.o_proj.bias', 'model.layers.69.self_attn.q_proj.bias', 'model.layers.69.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.70.self_attn.k_proj.bias', 'model.layers.70.self_attn.o_proj.bias', 'model.layers.70.self_attn.q_proj.bias', 'model.layers.70.self_attn.v_proj.bias', 'model.layers.71.self_attn.k_proj.bias', 'model.layers.71.self_attn.o_proj.bias', 'model.layers.71.self_attn.q_proj.bias', 'model.layers.71.self_attn.v_proj.bias', 'model.layers.72.self_attn.k_proj.bias', 'model.layers.72.self_attn.o_proj.bias', 'model.layers.72.self_attn.q_proj.bias', 'model.layers.72.self_attn.v_proj.bias', 'model.layers.73.self_attn.k_proj.bias', 'model.layers.73.self_attn.o_proj.bias', 'model.layers.73.self_attn.q_proj.bias', 'model.layers.73.self_attn.v_proj.bias', 'model.layers.74.self_attn.k_proj.bias', 'model.layers.74.self_attn.o_proj.bias', 'model.layers.74.self_attn.q_proj.bias', 'model.layers.74.self_attn.v_proj.bias', 'model.layers.75.self_attn.k_proj.bias', 'model.layers.75.self_attn.o_proj.bias', 'model.layers.75.self_attn.q_proj.bias', 'model.layers.75.self_attn.v_proj.bias', 'model.layers.76.self_attn.k_proj.bias', 'model.layers.76.self_attn.o_proj.bias', 'model.layers.76.self_attn.q_proj.bias', 'model.layers.76.self_attn.v_proj.bias', 'model.layers.77.self_attn.k_proj.bias', 'model.layers.77.self_attn.o_proj.bias', 'model.layers.77.self_attn.q_proj.bias', 'model.layers.77.self_attn.v_proj.bias', 'model.layers.78.self_attn.k_proj.bias', 'model.layers.78.self_attn.o_proj.bias', 'model.layers.78.self_attn.q_proj.bias', 'model.layers.78.self_attn.v_proj.bias', 'model.layers.79.self_attn.k_proj.bias', 'model.layers.79.self_attn.o_proj.bias', 'model.layers.79.self_attn.q_proj.bias', 'model.layers.79.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
get_com_directions:   0%|          | 0/80 [00:00<?, ?it/s]get_com_directions:   1%|▏         | 1/80 [00:00<00:08,  8.88it/s]get_com_directions:   2%|▎         | 2/80 [00:00<00:08,  8.67it/s]get_com_directions:   4%|▍         | 3/80 [00:00<00:08,  8.73it/s]get_com_directions:   5%|▌         | 4/80 [00:00<00:08,  8.81it/s]get_com_directions:   6%|▋         | 5/80 [00:00<00:08,  8.85it/s]get_com_directions:   8%|▊         | 6/80 [00:00<00:08,  8.93it/s]get_com_directions:   9%|▉         | 7/80 [00:00<00:08,  8.98it/s]get_com_directions:  10%|█         | 8/80 [00:00<00:07,  9.02it/s]get_com_directions:  11%|█▏        | 9/80 [00:01<00:07,  9.09it/s]get_com_directions:  12%|█▎        | 10/80 [00:01<00:07,  9.14it/s]get_com_directions:  14%|█▍        | 11/80 [00:01<00:07,  9.15it/s]get_com_directions:  15%|█▌        | 12/80 [00:01<00:07,  9.16it/s]get_com_directions:  16%|█▋        | 13/80 [00:01<00:07,  9.20it/s]get_com_directions:  18%|█▊        | 14/80 [00:01<00:07,  9.24it/s]get_com_directions:  19%|█▉        | 15/80 [00:01<00:06,  9.29it/s]get_com_directions:  20%|██        | 16/80 [00:01<00:06,  9.33it/s]get_com_directions:  21%|██▏       | 17/80 [00:01<00:06,  9.36it/s]get_com_directions:  22%|██▎       | 18/80 [00:01<00:06,  9.35it/s]get_com_directions:  24%|██▍       | 19/80 [00:02<00:06,  9.38it/s]get_com_directions:  25%|██▌       | 20/80 [00:02<00:06,  9.41it/s]get_com_directions:  26%|██▋       | 21/80 [00:02<00:06,  9.41it/s]get_com_directions:  28%|██▊       | 22/80 [00:02<00:06,  9.42it/s]get_com_directions:  29%|██▉       | 23/80 [00:02<00:06,  9.42it/s]get_com_directions:  30%|███       | 24/80 [00:02<00:05,  9.43it/s]get_com_directions:  31%|███▏      | 25/80 [00:02<00:05,  9.43it/s]get_com_directions:  32%|███▎      | 26/80 [00:02<00:05,  9.43it/s]get_com_directions:  34%|███▍      | 27/80 [00:02<00:05,  9.44it/s]get_com_directions:  35%|███▌      | 28/80 [00:03<00:05,  9.45it/s]get_com_directions:  36%|███▋      | 29/80 [00:03<00:05,  9.45it/s]get_com_directions:  38%|███▊      | 30/80 [00:03<00:05,  9.45it/s]get_com_directions:  39%|███▉      | 31/80 [00:03<00:05,  9.46it/s]get_com_directions:  40%|████      | 32/80 [00:03<00:05,  9.47it/s]get_com_directions:  41%|████▏     | 33/80 [00:03<00:04,  9.47it/s]get_com_directions:  42%|████▎     | 34/80 [00:03<00:04,  9.47it/s]get_com_directions:  44%|████▍     | 35/80 [00:03<00:04,  9.45it/s]get_com_directions:  45%|████▌     | 36/80 [00:03<00:04,  9.46it/s]get_com_directions:  46%|████▋     | 37/80 [00:03<00:04,  9.46it/s]get_com_directions:  48%|████▊     | 38/80 [00:04<00:04,  9.46it/s]get_com_directions:  49%|████▉     | 39/80 [00:04<00:04,  9.46it/s]get_com_directions:  50%|█████     | 40/80 [00:04<00:04,  9.46it/s]get_com_directions:  51%|█████▏    | 41/80 [00:04<00:04,  9.46it/s]get_com_directions:  52%|█████▎    | 42/80 [00:04<00:04,  9.45it/s]get_com_directions:  54%|█████▍    | 43/80 [00:04<00:03,  9.44it/s]get_com_directions:  55%|█████▌    | 44/80 [00:04<00:03,  9.45it/s]get_com_directions:  56%|█████▋    | 45/80 [00:04<00:03,  9.45it/s]get_com_directions:  57%|█████▊    | 46/80 [00:04<00:03,  9.44it/s]get_com_directions:  59%|█████▉    | 47/80 [00:05<00:03,  9.41it/s]get_com_directions:  60%|██████    | 48/80 [00:05<00:03,  9.40it/s]get_com_directions:  61%|██████▏   | 49/80 [00:05<00:03,  9.42it/s]get_com_directions:  62%|██████▎   | 50/80 [00:05<00:03,  9.42it/s]get_com_directions:  64%|██████▍   | 51/80 [00:05<00:03,  9.40it/s]get_com_directions:  65%|██████▌   | 52/80 [00:05<00:02,  9.41it/s]get_com_directions:  66%|██████▋   | 53/80 [00:05<00:02,  9.43it/s]get_com_directions:  68%|██████▊   | 54/80 [00:05<00:02,  9.43it/s]get_com_directions:  69%|██████▉   | 55/80 [00:05<00:02,  9.43it/s]get_com_directions:  70%|███████   | 56/80 [00:05<00:02,  9.43it/s]get_com_directions:  71%|███████▏  | 57/80 [00:06<00:02,  9.44it/s]get_com_directions:  72%|███████▎  | 58/80 [00:06<00:02,  9.43it/s]get_com_directions:  74%|███████▍  | 59/80 [00:06<00:02,  9.44it/s]get_com_directions:  75%|███████▌  | 60/80 [00:06<00:02,  9.43it/s]get_com_directions:  76%|███████▋  | 61/80 [00:06<00:02,  9.44it/s]get_com_directions:  78%|███████▊  | 62/80 [00:06<00:01,  9.44it/s]get_com_directions:  79%|███████▉  | 63/80 [00:06<00:01,  9.42it/s]get_com_directions:  80%|████████  | 64/80 [00:06<00:01,  9.41it/s]get_com_directions:  81%|████████▏ | 65/80 [00:06<00:01,  9.43it/s]get_com_directions:  82%|████████▎ | 66/80 [00:07<00:01,  9.39it/s]get_com_directions:  84%|████████▍ | 67/80 [00:07<00:01,  9.40it/s]get_com_directions:  85%|████████▌ | 68/80 [00:07<00:01,  9.41it/s]get_com_directions:  86%|████████▋ | 69/80 [00:07<00:01,  9.43it/s]get_com_directions:  88%|████████▊ | 70/80 [00:07<00:01,  9.43it/s]get_com_directions:  89%|████████▉ | 71/80 [00:07<00:00,  9.44it/s]get_com_directions:  90%|█████████ | 72/80 [00:07<00:00,  9.45it/s]get_com_directions:  91%|█████████▏| 73/80 [00:07<00:00,  9.46it/s]get_com_directions:  92%|█████████▎| 74/80 [00:07<00:00,  9.47it/s]get_com_directions:  94%|█████████▍| 75/80 [00:08<00:00,  9.48it/s]get_com_directions:  95%|█████████▌| 76/80 [00:08<00:00,  9.48it/s]get_com_directions:  96%|█████████▋| 77/80 [00:08<00:00,  9.49it/s]get_com_directions:  98%|█████████▊| 78/80 [00:08<00:00,  9.49it/s]get_com_directions:  99%|█████████▉| 79/80 [00:08<00:00,  9.50it/s]get_com_directions: 100%|██████████| 80/80 [00:08<00:00,  9.50it/s]get_com_directions: 100%|██████████| 80/80 [00:08<00:00,  9.37it/s]
train_probes:   0%|          | 0/80 [00:00<?, ?it/s]train_probes:   1%|▏         | 1/80 [00:00<00:40,  1.97it/s]train_probes:   2%|▎         | 2/80 [00:00<00:38,  2.02it/s]train_probes:   4%|▍         | 3/80 [00:01<00:38,  2.02it/s]train_probes:   5%|▌         | 4/80 [00:02<00:38,  1.98it/s]train_probes:   6%|▋         | 5/80 [00:02<00:38,  1.96it/s]train_probes:   8%|▊         | 6/80 [00:03<00:37,  2.00it/s]train_probes:   9%|▉         | 7/80 [00:03<00:36,  2.03it/s]train_probes:  10%|█         | 8/80 [00:03<00:35,  2.04it/s]train_probes:  11%|█▏        | 9/80 [00:04<00:35,  2.01it/s]train_probes:  12%|█▎        | 10/80 [00:05<00:35,  1.99it/s]train_probes:  14%|█▍        | 11/80 [00:05<00:34,  2.01it/s]train_probes:  15%|█▌        | 12/80 [00:05<00:33,  2.02it/s]train_probes:  16%|█▋        | 13/80 [00:06<00:33,  1.98it/s]train_probes:  18%|█▊        | 14/80 [00:07<00:36,  1.82it/s]train_probes:  19%|█▉        | 15/80 [00:07<00:38,  1.67it/s]train_probes:  20%|██        | 16/80 [00:08<00:40,  1.59it/s]train_probes:  21%|██▏       | 17/80 [00:09<00:41,  1.52it/s]train_probes:  22%|██▎       | 18/80 [00:10<00:41,  1.48it/s]train_probes:  24%|██▍       | 19/80 [00:10<00:42,  1.42it/s]train_probes:  25%|██▌       | 20/80 [00:11<00:40,  1.49it/s]train_probes:  26%|██▋       | 21/80 [00:12<00:40,  1.45it/s]train_probes:  28%|██▊       | 22/80 [00:12<00:40,  1.44it/s]train_probes:  29%|██▉       | 23/80 [00:13<00:40,  1.42it/s]train_probes:  30%|███       | 24/80 [00:14<00:40,  1.40it/s]train_probes:  31%|███▏      | 25/80 [00:15<00:39,  1.40it/s]train_probes:  32%|███▎      | 26/80 [00:15<00:38,  1.39it/s]train_probes:  34%|███▍      | 27/80 [00:16<00:34,  1.53it/s]train_probes:  35%|███▌      | 28/80 [00:16<00:33,  1.56it/s]train_probes:  36%|███▋      | 29/80 [00:17<00:32,  1.55it/s]train_probes:  38%|███▊      | 30/80 [00:18<00:32,  1.52it/s]train_probes:  39%|███▉      | 31/80 [00:18<00:31,  1.55it/s]train_probes:  40%|████      | 32/80 [00:19<00:28,  1.67it/s]train_probes:  41%|████▏     | 33/80 [00:19<00:27,  1.69it/s]train_probes:  42%|████▎     | 34/80 [00:20<00:26,  1.72it/s]train_probes:  44%|████▍     | 35/80 [00:21<00:26,  1.73it/s]train_probes:  45%|████▌     | 36/80 [00:21<00:25,  1.72it/s]train_probes:  46%|████▋     | 37/80 [00:22<00:25,  1.70it/s]train_probes:  48%|████▊     | 38/80 [00:22<00:25,  1.62it/s]train_probes:  49%|████▉     | 39/80 [00:23<00:25,  1.59it/s]train_probes:  50%|█████     | 40/80 [00:24<00:24,  1.63it/s]train_probes:  51%|█████▏    | 41/80 [00:24<00:23,  1.64it/s]train_probes:  52%|█████▎    | 42/80 [00:25<00:24,  1.55it/s]train_probes:  54%|█████▍    | 43/80 [00:26<00:23,  1.57it/s]train_probes:  55%|█████▌    | 44/80 [00:26<00:24,  1.49it/s]train_probes:  56%|█████▋    | 45/80 [00:27<00:22,  1.55it/s]train_probes:  57%|█████▊    | 46/80 [00:28<00:22,  1.51it/s]train_probes:  59%|█████▉    | 47/80 [00:28<00:20,  1.62it/s]train_probes:  60%|██████    | 48/80 [00:29<00:20,  1.58it/s]train_probes:  61%|██████▏   | 49/80 [00:29<00:19,  1.61it/s]train_probes:  62%|██████▎   | 50/80 [00:30<00:19,  1.54it/s]train_probes:  64%|██████▍   | 51/80 [00:31<00:18,  1.54it/s]train_probes:  65%|██████▌   | 52/80 [00:31<00:18,  1.48it/s]train_probes:  66%|██████▋   | 53/80 [00:32<00:17,  1.56it/s]train_probes:  68%|██████▊   | 54/80 [00:33<00:17,  1.49it/s]train_probes:  69%|██████▉   | 55/80 [00:33<00:17,  1.46it/s]train_probes:  70%|███████   | 56/80 [00:34<00:16,  1.42it/s]train_probes:  71%|███████▏  | 57/80 [00:35<00:16,  1.42it/s]train_probes:  72%|███████▎  | 58/80 [00:36<00:14,  1.47it/s]train_probes:  74%|███████▍  | 59/80 [00:36<00:14,  1.43it/s]train_probes:  75%|███████▌  | 60/80 [00:37<00:13,  1.48it/s]train_probes:  76%|███████▋  | 61/80 [00:38<00:12,  1.56it/s]train_probes:  78%|███████▊  | 62/80 [00:38<00:11,  1.58it/s]train_probes:  79%|███████▉  | 63/80 [00:39<00:10,  1.65it/s]train_probes:  80%|████████  | 64/80 [00:39<00:10,  1.57it/s]train_probes:  81%|████████▏ | 65/80 [00:40<00:10,  1.49it/s]train_probes:  82%|████████▎ | 66/80 [00:41<00:08,  1.63it/s]train_probes:  84%|████████▍ | 67/80 [00:41<00:08,  1.50it/s]train_probes:  85%|████████▌ | 68/80 [00:42<00:08,  1.43it/s]train_probes:  86%|████████▋ | 69/80 [00:43<00:07,  1.52it/s]train_probes:  88%|████████▊ | 70/80 [00:43<00:06,  1.45it/s]train_probes:  89%|████████▉ | 71/80 [00:44<00:06,  1.48it/s]train_probes:  90%|█████████ | 72/80 [00:45<00:05,  1.51it/s]train_probes:  91%|█████████▏| 73/80 [00:45<00:04,  1.54it/s]train_probes:  92%|█████████▎| 74/80 [00:46<00:03,  1.54it/s]train_probes:  94%|█████████▍| 75/80 [00:47<00:03,  1.54it/s]train_probes:  95%|█████████▌| 76/80 [00:47<00:02,  1.54it/s]train_probes:  96%|█████████▋| 77/80 [00:48<00:02,  1.45it/s]train_probes:  98%|█████████▊| 78/80 [00:49<00:01,  1.50it/s]train_probes:  99%|█████████▉| 79/80 [00:49<00:00,  1.47it/s]train_probes: 100%|██████████| 80/80 [00:50<00:00,  1.44it/s]train_probes: 100%|██████████| 80/80 [00:50<00:00,  1.58it/s]
tqa_run_answers:   0%|          | 0/409 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
tqa_run_answers:   0%|          | 1/409 [00:20<2:20:25, 20.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   0%|          | 2/409 [00:24<1:11:37, 10.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   1%|          | 3/409 [00:27<49:37,  7.33s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   1%|          | 4/409 [00:31<39:16,  5.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   1%|          | 5/409 [00:34<33:31,  4.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   1%|▏         | 6/409 [00:38<30:03,  4.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   2%|▏         | 7/409 [00:41<27:49,  4.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   2%|▏         | 8/409 [00:45<26:21,  3.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   2%|▏         | 9/409 [00:48<25:21,  3.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   2%|▏         | 10/409 [00:52<24:39,  3.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   3%|▎         | 11/409 [00:55<24:09,  3.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   3%|▎         | 12/409 [00:59<23:48,  3.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   3%|▎         | 13/409 [01:02<23:32,  3.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   3%|▎         | 14/409 [01:06<23:20,  3.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   4%|▎         | 15/409 [01:09<23:10,  3.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   4%|▍         | 16/409 [01:13<23:04,  3.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   4%|▍         | 17/409 [01:16<22:57,  3.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   4%|▍         | 18/409 [01:20<22:51,  3.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   5%|▍         | 19/409 [01:23<22:46,  3.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   5%|▍         | 20/409 [01:27<22:42,  3.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   5%|▌         | 21/409 [01:30<22:43,  3.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   5%|▌         | 22/409 [01:34<22:37,  3.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   6%|▌         | 23/409 [01:37<22:32,  3.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
tqa_run_answers:   6%|▌         | 24/409 [01:41<22:27,  3.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
